# Learn Machine Learning Algorithms from Scratch

A comprehensive educational project for understanding machine learning algorithms by implementing them from scratch using only NumPy. Perfect for beginners who want to understand the mathematical foundations and inner workings of ML algorithms.

**Created by [Ridwan Hall](https://ridwaanhall.com) for educational purposes.**

## ğŸ¯ Project Goals

- **Educational Focus**: Learn how ML algorithms work under the hood
- **Mathematical Understanding**: Implement algorithms using mathematical foundations
- **No Black Boxes**: Build everything from scratch (except NumPy for mathematical operations)
- **Best Practices**: Learn when and how to use different algorithms, optimizers, and loss functions
- **Practical Knowledge**: Understand combinations and use cases for different components

## ğŸ“ Project Structure

```
Learn-ML-Algorithm-from-Scratch/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/              # ML algorithms implementation
â”‚   â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â”‚   â”œâ”€â”€ neural_network.py
â”‚   â”‚   â”œâ”€â”€ decision_tree.py
â”‚   â”‚   â”œâ”€â”€ kmeans.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ optimization/        # Optimization algorithms
â”‚   â”‚   â”œâ”€â”€ gradient_descent.py
â”‚   â”‚   â”œâ”€â”€ adam.py
â”‚   â”‚   â”œâ”€â”€ rmsprop.py
â”‚   â”‚   â”œâ”€â”€ momentum.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ loss_functions/      # Loss functions
â”‚   â”‚   â”œâ”€â”€ mse.py
â”‚   â”‚   â”œâ”€â”€ cross_entropy.py
â”‚   â”‚   â”œâ”€â”€ mae.py
â”‚   â”‚   â”œâ”€â”€ huber.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ activation_functions/ # Activation functions
â”‚   â”‚   â”œâ”€â”€ sigmoid.py
â”‚   â”‚   â”œâ”€â”€ relu.py
â”‚   â”‚   â”œâ”€â”€ tanh.py
â”‚   â”‚   â”œâ”€â”€ softmax.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics/             # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ accuracy.py
â”‚   â”‚   â”œâ”€â”€ precision_recall.py
â”‚   â”‚   â”œâ”€â”€ f1_score.py
â”‚   â”‚   â”œâ”€â”€ r2_score.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing/       # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ scaler.py
â”‚   â”‚   â”œâ”€â”€ encoder.py
â”‚   â”‚   â”œâ”€â”€ splitter.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ utils/              # Utility functions
â”‚       â”œâ”€â”€ data_loader.py
â”‚       â”œâ”€â”€ visualizer.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ examples/               # Example implementations
â”œâ”€â”€ docs/                  # Detailed documentation
â”œâ”€â”€ tests/                 # Unit tests
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

1. **Clone and Setup**:
   ```bash
   git clone <repository-url>
   cd Learn-ML-Algorithm-from-Scratch
   pip install -r requirements.txt
   ```

2. **Run Your First Example**:
   ```python
   from src.models.linear_regression import LinearRegression
   from src.loss_functions.mse import MSE
   from src.optimization.gradient_descent import GradientDescent
   
   # Create model with components
   model = LinearRegression()
   loss_fn = MSE()
   optimizer = GradientDescent(learning_rate=0.01)
   
   # Train the model
   model.fit(X_train, y_train, loss_fn, optimizer, epochs=1000)
   ```

## ğŸ“š Learning Path

### 1. **Start with Fundamentals**
- **Loss Functions** (`docs/loss_functions.md`)
- **Activation Functions** (`docs/activation_functions.md`)
- **Optimization Algorithms** (`docs/optimization.md`)

### 2. **Simple Models**
- **Linear Regression** - Perfect for understanding gradient descent
- **Logistic Regression** - Introduction to classification

### 3. **Advanced Models**
- **Neural Networks** - Combining all concepts
- **Decision Trees** - Different approach to ML
- **K-Means** - Unsupervised learning

## ğŸ“ Educational Features

### Interactive Learning
- Each component has detailed docstrings explaining the math
- Step-by-step implementation with comments
- Visual examples in Jupyter notebooks

### Best Practices Guide
- **When to use Adam vs SGD**: See `docs/optimizer_guide.md`
- **Loss function selection**: See `docs/loss_function_guide.md`
- **Activation function combinations**: See `docs/activation_guide.md`

### Mathematical Foundations
- All algorithms implemented with clear mathematical derivations
- Gradient calculations shown step-by-step
- No hidden complexity - you see everything

## ğŸ”§ Key Components

### Models
- **Linear Regression**: Fundamental regression algorithm
- **Logistic Regression**: Binary and multiclass classification
- **Neural Network**: Fully connected networks with backpropagation
- **Decision Tree**: Tree-based learning algorithm
- **K-Means**: Clustering algorithm

### Optimizers
- **Gradient Descent**: Basic optimization
- **Adam**: Adaptive learning rate with momentum
- **RMSprop**: Adaptive learning rate
- **Momentum**: Enhanced gradient descent

### Loss Functions
- **MSE**: Mean Squared Error for regression
- **Cross Entropy**: For classification tasks
- **MAE**: Mean Absolute Error for robust regression
- **Huber**: Robust loss function

### Activation Functions
- **ReLU**: Most popular for hidden layers
- **Sigmoid**: For binary classification output
- **Tanh**: Alternative to sigmoid
- **Softmax**: For multiclass classification

## ğŸ“– Documentation

### ğŸš€ **Start Here** â†’ [Project Overview](docs/project_overview.md)
**Complete guide to learning machine learning from scratch with structured paths and clear progression**

### Getting Started
- **[Quick Start Guide](docs/quickstart.md)** - Get up and running in 5 minutes
- **[Complete Learning Guide](docs/complete_learning_guide.md)** - Structured 8-week learning path
- **[Best Practices Guide](docs/best_practices.md)** - Choose the right components

### Technical References  
- **[API Reference](docs/api_reference.md)** - Complete documentation of all classes and methods
- **[Mathematical Foundations](docs/mathematical_foundations.md)** - Deep dive into the math behind each algorithm
- **[Troubleshooting Guide](docs/troubleshooting.md)** - Debug training issues and fix common problems

### Educational Flow
1. **Start Here**: `docs/project_overview.md` - Understand the complete learning journey
2. **Quick Start**: `docs/quickstart.md` - Run your first model
3. **Understand**: `docs/mathematical_foundations.md` - Learn the math  
4. **Master**: `docs/complete_learning_guide.md` - Follow the structured path
5. **Apply**: `docs/best_practices.md` - Make good choices
6. **Debug**: `docs/troubleshooting.md` - Fix problems
7. **Reference**: `docs/api_reference.md` - Look up details

## ğŸ¯ Learning Outcomes

After completing this project, you will understand:

1. **How gradient descent actually works**
2. **Why certain optimizers work better for specific problems**
3. **How to choose appropriate loss functions**
4. **The mathematical foundations of neural networks**
5. **When to use different activation functions**
6. **How to implement ML algorithms from mathematical formulas**

## ğŸ‘¨â€ğŸ’» Author

**Ridwan Hall**  
Website: [ridwaanhall.com](https://ridwaanhall.com)

## ğŸ¤ Contributing

This is an educational project! Contributions that enhance learning are welcome:
- Better documentation and explanations
- More visual examples
- Additional algorithms
- Improved mathematical explanations

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

MIT License - Feel free to use this for educational purposes.

## ğŸ‰ Get Started

Check out the `examples/` directory for hands-on tutorials, or dive into the documentation in `docs/` to understand the theory behind each component.

Happy Learning! ğŸš€