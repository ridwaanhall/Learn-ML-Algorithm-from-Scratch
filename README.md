# ğŸ¤– Machine Learning from Scratch

A comprehensive Python project implementing core Machine Learning algorithms from scratch using Object-Oriented Programming principles. This educational framework helps you understand how ML algorithms work internally without relying on high-level libraries.

## ğŸ¯ Project Overview

This project implements a complete ML framework covering:
- **Supervised Learning**: Regression and Classification
- **Unsupervised Learning**: Clustering and Dimensionality Reduction
- **Optimization**: SGD and Adam optimizers
- **Preprocessing**: Data scaling, encoding, and splitting
- **Evaluation**: Comprehensive metrics for model assessment

## ğŸ“ Project Structure

```
ml_from_scratch/
â”œâ”€â”€ main.py                       # Entry point with comprehensive demos
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ iris.csv                  # Famous iris classification dataset
â”‚   â””â”€â”€ housing.csv               # Housing price regression dataset
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_model.py            # Abstract base classes
â”‚   â”œâ”€â”€ linear_regression.py      # Linear & Ridge Regression
â”‚   â”œâ”€â”€ logistic_regression.py    # Logistic Regression
â”‚   â”œâ”€â”€ decision_tree.py          # Decision Trees (CART)
â”‚   â”œâ”€â”€ knn.py                    # K-Nearest Neighbors
â”‚   â”œâ”€â”€ kmeans.py                 # K-Means Clustering
â”‚   â””â”€â”€ pca.py                    # Principal Component Analysis
â”œâ”€â”€ optimization/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sgd.py                    # Stochastic Gradient Descent
â”‚   â””â”€â”€ adam.py                   # Adam Optimizer
â”œâ”€â”€ loss_functions/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mse.py                    # Mean Squared Error
â”‚   â”œâ”€â”€ mae.py                    # Mean Absolute Error
â”‚   â””â”€â”€ cross_entropy.py         # Cross Entropy Loss
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classification.py        # Classification metrics
â”‚   â””â”€â”€ regression.py            # Regression metrics
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scaler.py                # Data scaling utilities
â”‚   â”œâ”€â”€ encoder.py               # Categorical encoding
â”‚   â””â”€â”€ split.py                 # Data splitting utilities
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ matrix.py                # Matrix operations & utilities
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd Learn-ML-Algorithm-from-Scratch
```

2. **Create virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

### Run the Demo

```bash
python main.py
```

This will run comprehensive demonstrations of all implemented algorithms.

## ğŸ”§ Core Components

### ğŸ¯ Supervised Learning

#### Linear Regression
```python
from models.linear_regression import LinearRegression

# Normal equation solver
model = LinearRegression(solver='normal_equation')
model.fit(X_train, y_train)
predictions = model.predict(X_test)

# SGD solver with custom parameters
model = LinearRegression(solver='sgd', learning_rate=0.01, max_iterations=1000)
```

#### Logistic Regression
```python
from models.logistic_regression import LogisticRegression

# Binary classification
model = LogisticRegression(max_iter=1000, learning_rate=0.1)
model.fit(X_train, y_train)

# Get probabilities
probabilities = model.predict_proba(X_test)
```

#### K-Nearest Neighbors
```python
from models.knn import KNNClassifier, KNNRegressor

# Classification
knn_clf = KNNClassifier(n_neighbors=5, weights='distance')
knn_clf.fit(X_train, y_train)

# Regression
knn_reg = KNNRegressor(n_neighbors=3, metric='euclidean')
knn_reg.fit(X_train, y_train)
```

#### Decision Trees
```python
from models.decision_tree import DecisionTreeClassifier, DecisionTreeRegressor

# Classification with Gini criterion
dt_clf = DecisionTreeClassifier(criterion='gini', max_depth=5)
dt_clf.fit(X_train, y_train)

# Regression with squared error
dt_reg = DecisionTreeRegressor(criterion='squared_error', max_depth=8)
dt_reg.fit(X_train, y_train)
```

### ğŸ” Unsupervised Learning

#### K-Means Clustering
```python
from models.kmeans import KMeans

# Basic clustering
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(X)

# Access cluster centers
centroids = kmeans.cluster_centers_
```

#### Principal Component Analysis
```python
from models.pca import PCA

# Keep 95% of variance
pca = PCA(n_components=0.95)
X_transformed = pca.fit_transform(X)

# 2D visualization
pca_2d = PCA(n_components=2)
X_2d = pca_2d.fit_transform(X)
```

### âš™ï¸ Preprocessing

#### Data Scaling
```python
from preprocessing.scaler import StandardScaler, MinMaxScaler

# Standardization (zero mean, unit variance)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Min-Max scaling (0-1 range)
minmax = MinMaxScaler(feature_range=(0, 1))
X_minmax = minmax.fit_transform(X)
```

#### Categorical Encoding
```python
from preprocessing.encoder import LabelEncoder, OneHotEncoder

# Label encoding for ordinal data
label_enc = LabelEncoder()
y_encoded = label_enc.fit_transform(y)

# One-hot encoding for nominal data
onehot_enc = OneHotEncoder()
X_encoded = onehot_enc.fit_transform(X_categorical)
```

#### Train-Test Split
```python
from preprocessing.split import TrainTestSplit

# Basic split
X_train, X_test, y_train, y_test = TrainTestSplit.train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Stratified split (maintains class distribution)
X_train, X_test, y_train, y_test = TrainTestSplit.train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)
```

### ğŸ“Š Evaluation Metrics

#### Classification Metrics
```python
from metrics.classification import ClassificationMetrics

# Basic metrics
accuracy = ClassificationMetrics.accuracy(y_true, y_pred)
precision = ClassificationMetrics.precision(y_true, y_pred)
recall = ClassificationMetrics.recall(y_true, y_pred)
f1 = ClassificationMetrics.f1_score(y_true, y_pred)

# Comprehensive report
report = ClassificationMetrics.classification_report(y_true, y_pred)
```

#### Regression Metrics
```python
from metrics.regression import RegressionMetrics

# Basic metrics
r2 = RegressionMetrics.r2_score(y_true, y_pred)
mse = RegressionMetrics.mean_squared_error(y_true, y_pred)
rmse = RegressionMetrics.root_mean_squared_error(y_true, y_pred)
mae = RegressionMetrics.mean_absolute_error(y_true, y_pred)

# Comprehensive report
report = RegressionMetrics.regression_report(y_true, y_pred, n_features=X.shape[1])
```

### ğŸ›ï¸ Optimization

#### SGD Optimizer
```python
from optimization.sgd import SGDOptimizer

optimizer = SGDOptimizer(learning_rate=0.01, momentum=0.9)
updated_params = optimizer.update(params, gradients)
```

#### Adam Optimizer
```python
from optimization.adam import AdamOptimizer

optimizer = AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)
updated_params = optimizer.update(params, gradients)
```

## ğŸ“ˆ Algorithms Implemented

### Supervised Learning
- **Linear Regression**: Normal equation and SGD solvers
- **Ridge Regression**: L2 regularization
- **Logistic Regression**: Binary and multiclass (one-vs-rest)
- **K-Nearest Neighbors**: Classification and regression variants
- **Decision Trees**: CART algorithm with Gini/Entropy criteria

### Unsupervised Learning
- **K-Means Clustering**: Lloyd's algorithm with k-means++ initialization
- **PCA**: Eigendecomposition-based dimensionality reduction

### Optimization
- **Stochastic Gradient Descent**: With momentum support
- **Adam**: Adaptive moment estimation optimizer

### Loss Functions
- **Mean Squared Error**: For regression tasks
- **Mean Absolute Error**: Robust regression loss
- **Cross Entropy**: For classification tasks

## ğŸ“ Educational Features

### Object-Oriented Design
- **Abstract Base Classes**: Consistent API across all models
- **Inheritance**: Specialized classes for different model types
- **Encapsulation**: Clean separation of concerns
- **Polymorphism**: Unified interface for different algorithms

### Documentation
- **Comprehensive Docstrings**: Every class and method documented
- **Type Hints**: Clear parameter and return types
- **Comments**: Explaining algorithmic details and mathematical concepts

### Code Quality
- **Modular Architecture**: Organized into logical packages
- **Error Handling**: Robust input validation
- **Consistent API**: Follows scikit-learn conventions
- **Clean Code**: Readable and maintainable implementation

## ğŸ” Example Usage

Here's a complete example using multiple components:

```python
import numpy as np
from models.logistic_regression import LogisticRegression
from preprocessing.scaler import StandardScaler
from preprocessing.split import TrainTestSplit
from metrics.classification import ClassificationMetrics

# Load your data
X, y = load_your_data()

# Split the data
X_train, X_test, y_train, y_test = TrainTestSplit.train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train model
model = LogisticRegression(learning_rate=0.1, max_iter=1000)
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)

# Evaluate
accuracy = ClassificationMetrics.accuracy(y_test, y_pred)
report = ClassificationMetrics.classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:", report)
```

## ğŸ¯ Learning Objectives

By exploring this codebase, you'll learn:

1. **Algorithm Implementation**: How ML algorithms work at a fundamental level
2. **Object-Oriented Design**: Best practices for structuring ML code
3. **Mathematical Foundations**: The math behind popular ML algorithms
4. **Software Engineering**: Clean, maintainable, and extensible code design
5. **Performance Considerations**: Numerical stability and computational efficiency

## ğŸ”§ Dependencies

- **NumPy**: For numerical computations and linear algebra
- **Pandas**: For data manipulation and CSV loading
- **Matplotlib**: For visualization in demonstrations

See `requirements.txt` for specific versions.

## ğŸ¤ Contributing

This is an educational project! Contributions are welcome:

1. **Algorithm Implementations**: Add new ML algorithms
2. **Optimizations**: Improve performance and numerical stability
3. **Documentation**: Enhance explanations and examples
4. **Testing**: Add unit tests and validation
5. **Features**: New preprocessing utilities or metrics

## ğŸ“š Further Learning

### Books
- "Pattern Recognition and Machine Learning" by Christopher Bishop
- "The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman
- "Machine Learning: A Probabilistic Perspective" by Kevin Murphy

### Online Resources
- [scikit-learn documentation](https://scikit-learn.org/) for API reference
- [Coursera ML Course](https://www.coursera.org/learn/machine-learning) by Andrew Ng
- [CS229 Stanford](http://cs229.stanford.edu/) for mathematical foundations

## ğŸ“„ License

This project is intended for educational purposes. Feel free to use, modify, and learn from the code.

## ğŸ‰ Acknowledgments

This implementation was inspired by:
- The scikit-learn API design
- Various educational ML resources and textbooks
- The need for understanding algorithms from first principles

---

**Happy Learning! ğŸš€**

Built with â¤ï¸ for machine learning education.
